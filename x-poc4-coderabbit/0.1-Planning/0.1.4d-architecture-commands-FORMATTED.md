# CodeRabbit Commands Reference
**Claude Code Integration - Command Guide**

**Document Type**: Architecture - Command Reference
**Created**: 2025-11-10
**Version**: 1.0
**Status**: Production Reference

---

## Overview

This document provides comprehensive command reference for using CodeRabbit with Claude Code, covering both CLI commands and natural language prompts.

**Two Usage Modes**:
1. **Terminal Commands**: Direct bash invocation (`coderabbit-json`)
2. **Natural Language**: Claude Code prompts ("Run CodeRabbit")

---

## Available Commands

### Primary Command: `coderabbit-json`

**Purpose**: Run CodeRabbit review with structured JSON output for Claude Code consumption

**Syntax**:
```bash
coderabbit-json [OPTIONS]
```

**Options**:
| Option | Description | Default | Example |
|--------|-------------|---------|---------|
| `--mode <mode>` | Review mode: `security`, `quality`, `all` | `all` | `--mode security` |
| `--path <path>` | Path to review | Current directory | `--path src/backend` |
| `--save-log` | Save output to DEFECT-LOG.md | false | `--save-log` |
| `--help`, `-h` | Show help message | N/A | `--help` |

**Examples**:
```bash
# Full review of current directory
coderabbit-json

# Security-focused review
coderabbit-json --mode security

# Review specific path and save to defect log
coderabbit-json --path src/backend --save-log

# Quality checks only
coderabbit-json --mode quality
```

**Output**:
- **stdout**: Structured JSON (for parsing)
- **stderr**: Human-readable status messages
- **Exit code**: 0 (success), 1 (critical issues or error)

**Exit Codes**:
```bash
# Check exit code
coderabbit-json
echo $?

# Exit codes:
# 0 = Success, no critical issues
# 1 = Critical issues found or error occurred
```

---

## Natural Language Usage in Claude Code

### Example 1: Quick Review

**You say**:
```
"Run CodeRabbit on my changes"
```

**Claude does**:
1. Runs: `coderabbit-json`
2. Parses JSON output
3. Responds:
```
CodeRabbit found 3 issues:

ðŸ”´ P0 (Critical): Hardcoded API key in src/auth.py:42
   Fix: Move to environment variable

ðŸŸ¡ P1 (High): Missing type hints in src/utils.py:15
   Fix: Add type annotations

âš« P2 (Medium): Function complexity in src/process.py:89
   Fix: Extract sub-functions
```

---

### Example 2: Auto-Fix Workflow

**You say**:
```
"Run CodeRabbit and fix all P0 and P1 issues"
```

**Claude does**:
1. Runs: `coderabbit-json`
2. Identifies P0 and P1 issues
3. For each issue:
   - Reads file and line number
   - Applies suggested fix
   - Commits change
4. Re-runs: `coderabbit-json`
5. Verifies fixes worked
6. Responds:
```
Fixed 2 critical issues:

âœ… src/auth.py:42 - Moved API key to environment variable
   Updated code to use os.getenv('API_KEY')

âœ… src/utils.py:15 - Added type hints
   Added annotations: def process_data(input: str, config: Config) -> Result

Re-ran CodeRabbit: âœ… No P0/P1 issues remain
```

---

### Example 3: Security-Focused Review

**You say**:
```
"Run a security scan with CodeRabbit and fix any vulnerabilities"
```

**Claude does**:
1. Runs: `coderabbit-json --mode security`
2. Focuses on security issues only
3. Auto-fixes all security problems
4. Responds with summary

---

### Example 4: Pre-Deployment Check

**You say**:
```
"Before I deploy, run a complete CodeRabbit review and make sure nothing is blocking"
```

**Claude does**:
1. Runs: `coderabbit-json --save-log`
2. Shows all issues
3. Highlights any P0 issues that block deployment
4. Saves to DEFECT-LOG.md
5. Responds:
```
Complete review finished:

Total: 5 issues
ðŸ”´ P0: 0 (âœ… None blocking deployment)
ðŸŸ¡ P1: 2 (Should fix before merge)
âš« P2: 3 (Can fix later)

Saved detailed report to DEFECT-LOG.md

âœ… Safe to deploy - no critical issues
```

---

## Prompts You Can Use

### During Development

**Quick checks**:
```
"Run CodeRabbit on my changes"
"Check for any issues before I commit"
"Do a quick security scan"
"Make sure my code follows Hana-X standards"
```

**Targeted reviews**:
```
"Run CodeRabbit on the backend code only"
"Check security in the authentication module"
"Review the API endpoints for issues"
```

---

### For Fixing

**Auto-fix commands**:
```
"Fix all the issues CodeRabbit found"
"Fix only P0 issues"
"Fix all critical and high priority issues"
"Apply the suggested fixes"
```

**Manual review**:
```
"Show me how to fix the P1 issues"
"Explain what the SOLID violation means"
"What's the best way to fix the complexity issue?"
```

---

### For Verification

**Post-fix checks**:
```
"Run CodeRabbit again to verify"
"Check if that fix worked"
"Make sure we're clean now"
"Verify no new issues were introduced"
```

---

### For Reporting

**Status and summaries**:
```
"What issues did CodeRabbit find?"
"Give me a summary of code quality"
"List all security issues"
"Save the findings to the defect log"
```

**Detailed reports**:
```
"Show me all P0 and P1 issues"
"What are the SOLID violations?"
"List issues by priority"
```

---

## Tips for Best Results

### 1. Review Early and Often

âœ… **Do**: Review after each feature
```
"I just implemented user authentication. Run CodeRabbit."
```

âŒ **Don't**: Wait until the end to review everything
```
"Review all my code from the last 2 weeks"  # Too late!
```

---

### 2. Fix Critical Issues Immediately

âœ… **Do**: Fix P0 issues right away
```
"Fix all P0 issues right now"
```

âŒ **Don't**: Defer critical fixes
```
"I'll fix those P0 issues later"  # Bad practice!
```

---

### 3. Let Claude Do the Fixing

âœ… **Do**: Use auto-fix for routine issues
```
"Fix the issues CodeRabbit found"
```

âŒ **Don't**: Manually fix when Claude can do it
```
*Manually editing files instead of asking Claude*
```

**Rationale**: Claude can:
- Read the exact file and line
- Apply suggested fixes accurately
- Verify the fix worked
- Re-run checks automatically

---

### 4. Verify After Fixes

âœ… **Do**: Always verify fixes worked
```
"Run CodeRabbit again to verify my fixes"
```

âŒ **Don't**: Assume fixes worked without checking
```
*Commits without verification*
```

---

### 5. Use Specific Commands

âœ… **Do**: Be specific about what you want
```
"Run security scan with CodeRabbit"
"Check SOLID compliance in the services layer"
"Review type hints in all Python files"
```

âŒ **Don't**: Use vague commands
```
"Check security"  # Too vague
"Make it better"  # What does "better" mean?
```

---

## Integration with Roger

For advanced workflows, you can also use Roger (the full orchestration agent):

```bash
# Roger orchestrates CodeRabbit
roger review --auto-fix

# Roger tracks defects across sessions
roger status
roger report
```

**Difference**:
- `coderabbit-json`: Direct, lightweight, for Claude Code
- `roger review`: Full orchestration, background processing, history tracking

**Use `coderabbit-json` when**: Working interactively in Claude Code
**Use `roger` when**: CI/CD, scheduled reviews, comprehensive tracking

---

## Understanding Output

### Issue Priorities

| Priority | Meaning | Action Required |
|----------|---------|-----------------|
| ðŸ”´ P0 | Critical | Must fix before deployment |
| ðŸŸ¡ P1 | High | Should fix before merge |
| âš« P2 | Medium | Fix when convenient |
| âšª P3 | Low | Nice to have |

### Issue Types

| Type | Example | Fix Time |
|------|---------|----------|
| Security | Hardcoded secrets, SQL injection | Immediate |
| SOLID Violation | Single Responsibility breach | 15-30 min |
| Code Quality | Missing type hints | 5-10 min |
| Documentation | Missing docstrings | 5 min |
| Testing | Low coverage | Varies |
| Performance | High complexity | 15-30 min |

---

## JSON Output Structure

When you invoke `coderabbit-json`, you get structured output like this:

```json
{
  "status": "completed",
  "total_issues": 3,
  "critical_issues": 1,
  "high_issues": 1,
  "medium_issues": 1,
  "low_issues": 0,
  "issues": [
    {
      "id": "DEF-001",
      "priority": "P0",
      "type": "security",
      "file": "src/auth.py",
      "line": 42,
      "message": "Hardcoded API key detected",
      "description": "API_KEY = 'sk-1234...' found in source code",
      "suggested_fix": "Move sensitive data to environment variables. Use .env file and load with os.getenv().",
      "reference": "Hana-X Standards: Section 4.2 - Security"
    }
  ],
  "summary": "Found 3 issues: ðŸ”´ 1 critical (P0) | ðŸŸ¡ 1 high (P1) | âš« 1 medium (P2) | âš ï¸  Critical issues must be fixed before deployment."
}
```

**Key Fields**:
- `status`: "completed" or "error"
- `total_issues`: Total number of issues found
- `critical_issues`: Number of P0 issues (blocks deployment)
- `issues[]`: Array of issue objects with file, line, fix suggestions
- `summary`: Human-readable summary

**Claude Code Uses This To**:
- Parse exact file locations (`issues[].file`, `issues[].line`)
- Understand priority (`issues[].priority`)
- Apply suggested fixes (`issues[].suggested_fix`)
- Reference Hana-X standards (`issues[].reference`)

---

## Advanced Usage Patterns

### Pattern 1: Incremental Fixes

```
User: "Implement user login feature"
Claude: [writes code]
User: "Run CodeRabbit"
Claude: [finds 5 issues]
User: "Fix P0 and P1 first"
Claude: [fixes critical issues]
User: "Now fix the documentation issues"
Claude: [fixes P2 issues]
User: "Verify everything is clean"
Claude: [runs review, confirms all fixed]
```

### Pattern 2: Security-First Development

```
User: "I'm implementing payment processing"
Claude: [writes payment code]
User: "Before we continue, run a security scan"
Claude: [runs coderabbit-json --mode security]
User: "Fix all security issues immediately"
Claude: [auto-fixes security problems]
User: "Now add the business logic"
```

### Pattern 3: Pre-Commit Workflow

```
User: "I'm ready to commit these changes"
Claude: "Let me run CodeRabbit first"
Claude: [runs review automatically]
Claude: "Found 2 P1 issues. Should I fix them before commit?"
User: "Yes, fix them"
Claude: [fixes issues, re-verifies, commits]
```

---

## Troubleshooting

### Issue: "CodeRabbit CLI not found"

**Error**:
```
Error: CodeRabbit CLI not found
Install with: curl -fsSL https://cli.coderabbit.ai/install.sh | sh
```

**Solution**:
```bash
# Install CodeRabbit CLI
curl -fsSL https://cli.coderabbit.ai/install.sh | sh

# Verify installation
coderabbit --version
```

---

### Issue: "Parser not found"

**Error**:
```
Error: Parser not found at /srv/cc/hana-x-infrastructure/bin/parse-coderabbit.py
```

**Solution**:
Ensure Phase 1 infrastructure is deployed:
```bash
ls -la /srv/cc/hana-x-infrastructure/bin/parse-coderabbit.py
```

---

### Issue: Exit code always 1

**Symptom**: Command exits with code 1 even when no issues found

**Check**:
```bash
# Run with verbose output
coderabbit-json 2>&1 | tee output.log

# Check JSON structure
coderabbit-json | jq .
```

**Solution**: Verify parser is correctly detecting issue counts

---

## Best Practices Summary

1. **Review Early**: Run CodeRabbit after implementing each feature
2. **Fix P0 Immediately**: Critical issues block deployment
3. **Use Auto-Fix**: Let Claude apply routine fixes
4. **Verify Fixes**: Always re-run CodeRabbit after fixes
5. **Be Specific**: Use targeted commands for better results
6. **Check Before Commit**: Always review before committing
7. **Save to Log**: Use `--save-log` for important reviews
8. **Learn from Issues**: Understand WHY issues occur, not just how to fix them

---

## Quick Reference Card

```bash
# Quick Commands
coderabbit-json                    # Full review
coderabbit-json --mode security   # Security only
coderabbit-json --save-log        # Save to DEFECT-LOG.md
roger review --auto-fix           # Full orchestration

# Natural Language (Claude Code)
"Run CodeRabbit"                  # Quick review
"Fix all P0 issues"               # Auto-fix critical
"Security scan please"            # Security focus
"Verify we're clean"              # Post-fix check
```

---

**Document Version**: 1.0
**Classification**: Internal - Reference
**Status**: Production
**Next Review**: After Phase 1 deployment

---

*Quality = Frequent reviews > Occasional deep dives*
*Automation = Claude fixes routine > Manual fixes*
